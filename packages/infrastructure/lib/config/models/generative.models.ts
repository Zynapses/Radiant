/**
 * 3D & Generative Models - 3D reconstruction, self-hosted LLMs
 */

import { SageMakerModelConfig } from './index';

export const GENERATIVE_3D_MODELS: SageMakerModelConfig[] = [
  {
    id: 'nerfstudio',
    name: 'nerfstudio',
    displayName: 'Nerfstudio',
    description: 'Neural Radiance Fields for 3D scene reconstruction',
    category: 'generative_3d',
    specialty: '3d_reconstruction',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.4xlarge',
    environment: { MODEL_NAME: 'nerfstudio', NERFSTUDIO_METHOD: 'nerfacto' },
    parameters: 0,
    capabilities: ['3d_reconstruction', 'novel_view_synthesis', 'scene_capture'],
    inputFormats: ['video/mp4', 'image/jpeg', 'image/png'],
    outputFormats: ['model/gltf+json', 'model/obj', 'video/mp4'],
    thermal: { defaultState: 'OFF', scaleToZeroAfterMinutes: 10, warmupTimeSeconds: 120, minInstances: 0, maxInstances: 2 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 3.55, per3DModel: 5.00, markup: 0.75 },
    minTier: 4, requiresGPU: true, gpuMemoryGB: 24, status: 'active',
  },
  {
    id: '3d-gaussian-splatting',
    name: '3d-gaussian-splatting',
    displayName: '3D Gaussian Splatting',
    description: 'Real-time radiance field rendering with 3D Gaussians',
    category: 'generative_3d',
    specialty: '3d_reconstruction',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.4xlarge',
    environment: { MODEL_NAME: '3dgs' },
    parameters: 0,
    capabilities: ['3d_reconstruction', 'real_time_rendering', 'novel_view_synthesis'],
    inputFormats: ['video/mp4', 'image/jpeg', 'image/png'],
    outputFormats: ['model/ply', 'model/gltf+json', 'video/mp4'],
    thermal: { defaultState: 'OFF', scaleToZeroAfterMinutes: 10, warmupTimeSeconds: 120, minInstances: 0, maxInstances: 2 },
    license: 'Custom',
    commercialUseNotes: 'Check license for commercial use terms',
    pricing: { hourlyRate: 3.55, per3DModel: 4.00, markup: 0.75 },
    minTier: 4, requiresGPU: true, gpuMemoryGB: 24, status: 'active',
  },
];

export const TEXT_GENERATION_MODELS: SageMakerModelConfig[] = [
  {
    id: 'mistral-7b-instruct',
    name: 'mistral-7b-instruct',
    displayName: 'Mistral 7B Instruct',
    description: 'Efficient instruction-following model',
    category: 'llm_text',
    specialty: 'text_generation',
    image: 'huggingface-pytorch-tgi-inference:2.1-tgi1.4-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.xlarge',
    environment: { HF_MODEL_ID: 'mistralai/Mistral-7B-Instruct-v0.3', MAX_INPUT_LENGTH: '4096', MAX_TOTAL_TOKENS: '8192' },
    parameters: 7_000_000_000,
    capabilities: ['text_generation', 'instruction_following', 'function_calling'],
    inputFormats: ['application/json', 'text/plain'],
    outputFormats: ['application/json', 'text/plain'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 90, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 2.47, perRequest: 0.005, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 16, status: 'active',
  },
  {
    id: 'llama-3-70b-instruct',
    name: 'llama-3-70b-instruct',
    displayName: 'Llama 3 70B Instruct',
    description: 'Large open-weight model for complex tasks',
    category: 'llm_text',
    specialty: 'text_generation',
    image: 'huggingface-pytorch-tgi-inference:2.1-tgi1.4-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.48xlarge',
    environment: { HF_MODEL_ID: 'meta-llama/Meta-Llama-3-70B-Instruct', MAX_INPUT_LENGTH: '8192', MAX_TOTAL_TOKENS: '16384', QUANTIZE: 'bitsandbytes' },
    parameters: 70_000_000_000,
    capabilities: ['text_generation', 'instruction_following', 'reasoning', 'code_generation'],
    inputFormats: ['application/json', 'text/plain'],
    outputFormats: ['application/json', 'text/plain'],
    thermal: { defaultState: 'OFF', scaleToZeroAfterMinutes: 10, warmupTimeSeconds: 300, minInstances: 0, maxInstances: 2 },
    license: 'Llama 3 Community License',
    commercialUseNotes: 'Free for commercial use, attribution required',
    pricing: { hourlyRate: 35.63, perRequest: 0.05, markup: 0.75 },
    minTier: 5, requiresGPU: true, gpuMemoryGB: 160, status: 'active',
  },
  {
    id: 'qwen-72b-instruct',
    name: 'qwen-72b-instruct',
    displayName: 'Qwen 2.5 72B Instruct',
    description: 'State-of-the-art multilingual model',
    category: 'llm_text',
    specialty: 'text_generation',
    image: 'huggingface-pytorch-tgi-inference:2.1-tgi1.4-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.48xlarge',
    environment: { HF_MODEL_ID: 'Qwen/Qwen2.5-72B-Instruct', MAX_INPUT_LENGTH: '32768', MAX_TOTAL_TOKENS: '65536', QUANTIZE: 'gptq' },
    parameters: 72_000_000_000,
    capabilities: ['text_generation', 'instruction_following', 'multilingual', 'long_context', 'code_generation'],
    inputFormats: ['application/json', 'text/plain'],
    outputFormats: ['application/json', 'text/plain'],
    thermal: { defaultState: 'OFF', scaleToZeroAfterMinutes: 10, warmupTimeSeconds: 300, minInstances: 0, maxInstances: 2 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 35.63, perRequest: 0.05, markup: 0.75 },
    minTier: 5, requiresGPU: true, gpuMemoryGB: 160, status: 'active',
  },
];

export const ALL_GENERATIVE_MODELS = [...GENERATIVE_3D_MODELS, ...TEXT_GENERATION_MODELS];
