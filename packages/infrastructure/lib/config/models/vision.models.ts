/**
 * Computer Vision Models - Classification, Detection, Segmentation
 */

import { SageMakerModelConfig } from './index';

// ============================================================================
// IMAGE CLASSIFICATION MODELS (4 models)
// ============================================================================

export const CLASSIFICATION_MODELS: SageMakerModelConfig[] = [
  {
    id: 'efficientnet-b0',
    name: 'efficientnet-b0',
    displayName: 'EfficientNet-B0',
    description: 'Lightweight image classification model',
    category: 'vision_classification',
    specialty: 'image_classification',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g4dn.xlarge',
    environment: { HF_MODEL_ID: 'google/efficientnet-b0', HF_TASK: 'image-classification' },
    parameters: 5_300_000,
    accuracy: '77.1% ImageNet Top-1',
    capabilities: ['image_classification', 'feature_extraction'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 45, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 1.30, perImage: 0.001, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 2, status: 'active',
  },
  {
    id: 'efficientnetv2-l',
    name: 'efficientnetv2-l',
    displayName: 'EfficientNetV2-L',
    description: 'State-of-the-art classification with improved training efficiency',
    category: 'vision_classification',
    specialty: 'image_classification',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.2xlarge',
    environment: { HF_MODEL_ID: 'google/efficientnetv2-l', HF_TASK: 'image-classification' },
    parameters: 118_000_000,
    accuracy: '85.7% ImageNet Top-1',
    capabilities: ['image_classification', 'feature_extraction'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 90, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 2.66, perImage: 0.003, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 10, status: 'active',
  },
  {
    id: 'swin-large',
    name: 'swin-large',
    displayName: 'Swin Transformer Large',
    description: 'Vision Transformer with shifted window attention',
    category: 'vision_classification',
    specialty: 'image_classification',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.2xlarge',
    environment: { HF_MODEL_ID: 'microsoft/swin-large-patch4-window7-224', HF_TASK: 'image-classification' },
    parameters: 197_000_000,
    accuracy: '87.3% ImageNet Top-1',
    capabilities: ['image_classification', 'feature_extraction'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 90, minInstances: 0, maxInstances: 3 },
    license: 'MIT',
    pricing: { hourlyRate: 2.66, perImage: 0.004, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 16, status: 'active',
  },
  {
    id: 'clip-vit-l14',
    name: 'clip-vit-l14',
    displayName: 'CLIP ViT-L/14',
    description: 'Multimodal vision-language model for zero-shot classification',
    category: 'vision_classification',
    specialty: 'image_classification',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.2xlarge',
    environment: { HF_MODEL_ID: 'openai/clip-vit-large-patch14', HF_TASK: 'zero-shot-image-classification' },
    parameters: 428_000_000,
    accuracy: '76.2% ImageNet Zero-Shot',
    capabilities: ['zero_shot_classification', 'image_text_matching'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 90, minInstances: 0, maxInstances: 3 },
    license: 'MIT',
    pricing: { hourlyRate: 2.66, perImage: 0.005, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 16, status: 'active',
  },
];

// ============================================================================
// OBJECT DETECTION MODELS (6 models)
// ============================================================================

export const DETECTION_MODELS: SageMakerModelConfig[] = [
  {
    id: 'yolov8n',
    name: 'yolov8n',
    displayName: 'YOLOv8 Nano',
    description: 'Ultra-lightweight real-time object detection',
    category: 'vision_detection',
    specialty: 'object_detection',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g4dn.xlarge',
    environment: { MODEL_NAME: 'yolov8n' },
    parameters: 3_200_000,
    benchmark: '37.3 COCO mAP',
    capabilities: ['object_detection', 'real_time'],
    inputFormats: ['image/jpeg', 'image/png', 'video/mp4'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 30, minInstances: 0, maxInstances: 5 },
    license: 'AGPL-3.0',
    commercialUseNotes: 'Requires Ultralytics Enterprise License for commercial use',
    pricing: { hourlyRate: 1.30, perImage: 0.002, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 2, status: 'active',
  },
  {
    id: 'yolov8m',
    name: 'yolov8m',
    displayName: 'YOLOv8 Medium',
    description: 'Medium model for balanced performance',
    category: 'vision_detection',
    specialty: 'object_detection',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.xlarge',
    environment: { MODEL_NAME: 'yolov8m' },
    parameters: 25_900_000,
    benchmark: '50.2 COCO mAP',
    capabilities: ['object_detection', 'real_time'],
    inputFormats: ['image/jpeg', 'image/png', 'video/mp4'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 45, minInstances: 0, maxInstances: 5 },
    license: 'AGPL-3.0',
    commercialUseNotes: 'Requires Ultralytics Enterprise License for commercial use',
    pricing: { hourlyRate: 2.47, perImage: 0.004, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 8, status: 'active',
  },
  {
    id: 'yolov8x',
    name: 'yolov8x',
    displayName: 'YOLOv8 Extra Large',
    description: 'Largest YOLOv8 for maximum accuracy',
    category: 'vision_detection',
    specialty: 'object_detection',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.xlarge',
    environment: { MODEL_NAME: 'yolov8x' },
    parameters: 68_200_000,
    benchmark: '53.9 COCO mAP',
    capabilities: ['object_detection', 'high_accuracy'],
    inputFormats: ['image/jpeg', 'image/png', 'video/mp4'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 60, minInstances: 0, maxInstances: 3 },
    license: 'AGPL-3.0',
    commercialUseNotes: 'Requires Ultralytics Enterprise License for commercial use',
    pricing: { hourlyRate: 2.47, perImage: 0.006, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 12, status: 'active',
  },
  {
    id: 'yolov11x',
    name: 'yolov11x',
    displayName: 'YOLOv11 Extra Large',
    description: 'Latest YOLO with improved architecture',
    category: 'vision_detection',
    specialty: 'object_detection',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.xlarge',
    environment: { MODEL_NAME: 'yolo11x' },
    parameters: 40_000_000,
    benchmark: '54.7 COCO mAP',
    capabilities: ['object_detection', 'real_time', 'high_accuracy'],
    inputFormats: ['image/jpeg', 'image/png', 'video/mp4'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 60, minInstances: 0, maxInstances: 3 },
    license: 'AGPL-3.0',
    commercialUseNotes: 'Requires Ultralytics Enterprise License for commercial use',
    pricing: { hourlyRate: 2.47, perImage: 0.006, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 10, status: 'active',
  },
  {
    id: 'rt-detr-x',
    name: 'rt-detr-x',
    displayName: 'RT-DETR-X',
    description: 'Real-time Detection Transformer (no NMS)',
    category: 'vision_detection',
    specialty: 'object_detection',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.xlarge',
    environment: { HF_MODEL_ID: 'PekingU/rtdetr_r101vd', HF_TASK: 'object-detection' },
    parameters: 40_000_000,
    benchmark: '54.8 COCO mAP',
    capabilities: ['object_detection', 'real_time', 'end_to_end'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 60, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 2.47, perImage: 0.005, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 10, status: 'active',
  },
  {
    id: 'grounding-dino',
    name: 'grounding-dino',
    displayName: 'Grounding DINO',
    description: 'Open-vocabulary object detection with text prompts',
    category: 'vision_detection',
    specialty: 'object_detection',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.2xlarge',
    environment: { HF_MODEL_ID: 'IDEA-Research/grounding-dino-base', HF_TASK: 'zero-shot-object-detection' },
    parameters: 172_000_000,
    benchmark: '52.5 COCO Zero-Shot',
    capabilities: ['zero_shot_detection', 'text_prompted', 'open_vocabulary'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 90, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 2.66, perImage: 0.008, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 16, status: 'active',
  },
];

// ============================================================================
// SEGMENTATION MODELS (4 models)
// ============================================================================

export const SEGMENTATION_MODELS: SageMakerModelConfig[] = [
  {
    id: 'sam-vit-h',
    name: 'sam-vit-h',
    displayName: 'SAM ViT-H',
    description: 'Segment Anything Model - largest variant',
    category: 'vision_segmentation',
    specialty: 'instance_segmentation',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.4xlarge',
    environment: { HF_MODEL_ID: 'facebook/sam-vit-huge', HF_TASK: 'mask-generation' },
    parameters: 636_000_000,
    capabilities: ['instance_segmentation', 'interactive', 'zero_shot'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json', 'image/png'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 120, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 3.55, perImage: 0.015, markup: 0.75 },
    minTier: 4, requiresGPU: true, gpuMemoryGB: 20, status: 'active',
  },
  {
    id: 'sam2',
    name: 'sam2',
    displayName: 'SAM 2',
    description: 'Segment Anything Model 2 with video support',
    category: 'vision_segmentation',
    specialty: 'instance_segmentation',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.4xlarge',
    environment: { MODEL_NAME: 'sam2_hiera_large' },
    parameters: 224_000_000,
    capabilities: ['instance_segmentation', 'video_segmentation', 'interactive', 'zero_shot'],
    inputFormats: ['image/jpeg', 'image/png', 'video/mp4'],
    outputFormats: ['application/json', 'image/png', 'video/mp4'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 120, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 3.55, perImage: 0.012, perMinuteVideo: 0.50, markup: 0.75 },
    minTier: 4, requiresGPU: true, gpuMemoryGB: 16, status: 'active',
  },
  {
    id: 'mobilesam',
    name: 'mobilesam',
    displayName: 'MobileSAM',
    description: 'Lightweight SAM for fast inference',
    category: 'vision_segmentation',
    specialty: 'instance_segmentation',
    image: 'pytorch-inference:2.1-transformers4.36-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g4dn.xlarge',
    environment: { HF_MODEL_ID: 'dhkim2810/mobilesam', HF_TASK: 'mask-generation' },
    parameters: 10_000_000,
    capabilities: ['instance_segmentation', 'interactive', 'fast'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json', 'image/png'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 30, minInstances: 0, maxInstances: 5 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 1.30, perImage: 0.004, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 4, status: 'active',
  },
  {
    id: 'mask-rcnn',
    name: 'mask-rcnn',
    displayName: 'Mask R-CNN',
    description: 'Classic instance segmentation model',
    category: 'vision_segmentation',
    specialty: 'instance_segmentation',
    image: 'pytorch-inference:2.1-gpu-py310-cu121-ubuntu22.04',
    instanceType: 'ml.g5.xlarge',
    environment: { MODEL_NAME: 'mask_rcnn_R_101_FPN_3x', DETECTRON2_CONFIG: 'COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml' },
    parameters: 63_000_000,
    benchmark: '42.9 COCO AP',
    capabilities: ['instance_segmentation', 'object_detection'],
    inputFormats: ['image/jpeg', 'image/png'],
    outputFormats: ['application/json', 'image/png'],
    thermal: { defaultState: 'COLD', scaleToZeroAfterMinutes: 15, warmupTimeSeconds: 60, minInstances: 0, maxInstances: 3 },
    license: 'Apache-2.0',
    pricing: { hourlyRate: 2.47, perImage: 0.006, markup: 0.75 },
    minTier: 3, requiresGPU: true, gpuMemoryGB: 8, status: 'active',
  },
];

// Export all vision models
export const ALL_VISION_MODELS = [...CLASSIFICATION_MODELS, ...DETECTION_MODELS, ...SEGMENTATION_MODELS];
