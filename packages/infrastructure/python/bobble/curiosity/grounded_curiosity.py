"""
Bobble Grounded Curiosity Engine

Implements autonomous curiosity with mandatory 20%+ tool grounding.
Prevents hallucination cementing by verifying against external reality.

See: /docs/bobble/adr/003-tool-grounding.md
"""

import asyncio
import random
from enum import Enum
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class ClaimType(Enum):
    """Types of claims that can be verified."""
    FACTUAL = "factual_claim"           # "The population of X is Y"
    NUMERICAL = "numerical_claim"        # "X costs $Y"
    TEMPORAL = "temporal_claim"          # "X happened in Y"
    ATTRIBUTION = "attribution"          # "X said Y"
    SCIENTIFIC = "scientific_claim"      # "Studies show X"
    GENERAL = "general_knowledge"        # General facts
    REASONING = "reasoning_chain"        # Logical reasoning
    CREATIVE = "creative_content"        # Creative/opinion
    OPINION = "opinion"                  # Subjective
    HYPOTHETICAL = "hypothetical"        # What-if scenarios
    META = "meta_statement"              # Self-referential


class GroundingTool(Enum):
    """Available grounding tools."""
    WEB_SEARCH = "web_search"
    CODE_EXECUTION = "code_execution"
    API_CALL = "api_call"
    DATABASE_QUERY = "database_query"
    DOCUMENT_RETRIEVAL = "document_retrieval"


@dataclass
class GroundingResult:
    """Result from a grounding operation."""
    tool: GroundingTool
    query: str
    result: str
    sources: List[str]
    confidence: float
    timestamp: datetime
    latency_ms: float


@dataclass
class CuriosityQuestion:
    """A question generated by the curiosity engine."""
    question_id: str
    question: str
    domain: str
    claim_type: ClaimType
    requires_grounding: bool
    priority: float
    created_at: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LearningOutcome:
    """Outcome from a curiosity loop."""
    question: CuriosityQuestion
    prediction: str
    prediction_confidence: float
    grounded: bool
    grounding_result: Optional[GroundingResult]
    nli_label: str
    nli_confidence: float
    surprise_score: float
    learning_progress: float
    cost: float


class GroundingPolicy:
    """
    Determines when to use external grounding.
    
    At least 20% of curiosity loops MUST use tool grounding
    to prevent hallucination cementing.
    """
    
    # Claim types that ALWAYS require grounding
    ALWAYS_GROUND = [
        ClaimType.FACTUAL,
        ClaimType.NUMERICAL,
        ClaimType.TEMPORAL,
        ClaimType.ATTRIBUTION,
        ClaimType.SCIENTIFIC,
    ]
    
    # Claim types that are sampled for grounding
    SAMPLE_GROUND = {
        ClaimType.GENERAL: 0.20,      # 20% sampling
        ClaimType.REASONING: 0.10,    # 10% sampling
        ClaimType.CREATIVE: 0.05,     # 5% sampling
    }
    
    # Claim types that never need grounding
    NEVER_GROUND = [
        ClaimType.OPINION,
        ClaimType.HYPOTHETICAL,
        ClaimType.META,
    ]
    
    def __init__(self, min_grounding_ratio: float = 0.20):
        self.min_grounding_ratio = min_grounding_ratio
        self.grounding_count = 0
        self.total_count = 0
    
    def should_ground(self, claim_type: ClaimType) -> bool:
        """Determine if a claim should be grounded."""
        self.total_count += 1
        
        # Always ground certain types
        if claim_type in self.ALWAYS_GROUND:
            self.grounding_count += 1
            return True
        
        # Never ground certain types
        if claim_type in self.NEVER_GROUND:
            return False
        
        # Sample grounding for other types
        sample_rate = self.SAMPLE_GROUND.get(claim_type, 0.10)
        
        # Increase sample rate if we're below minimum
        current_ratio = self.grounding_count / max(self.total_count, 1)
        if current_ratio < self.min_grounding_ratio:
            sample_rate = min(1.0, sample_rate * 2)
        
        if random.random() < sample_rate:
            self.grounding_count += 1
            return True
        
        return False
    
    def get_grounding_ratio(self) -> float:
        """Get current grounding ratio."""
        if self.total_count == 0:
            return 0.0
        return self.grounding_count / self.total_count
    
    def reset_counters(self):
        """Reset counters (call at start of night mode)."""
        self.grounding_count = 0
        self.total_count = 0


class LearningProgressTracker:
    """
    Tracks learning progress across domains.
    
    Learning progress = rate of prediction improvement over time.
    High LP domains are prioritized for exploration.
    """
    
    def __init__(self):
        self.domain_history: Dict[str, List[float]] = {}
        self.domain_lp: Dict[str, float] = {}
    
    def record_outcome(self, domain: str, surprise: float):
        """Record learning outcome for a domain."""
        if domain not in self.domain_history:
            self.domain_history[domain] = []
        
        self.domain_history[domain].append(surprise)
        
        # Keep last 100 outcomes per domain
        if len(self.domain_history[domain]) > 100:
            self.domain_history[domain] = self.domain_history[domain][-100:]
        
        # Calculate learning progress (reduction in surprise over time)
        self._update_lp(domain)
    
    def _update_lp(self, domain: str):
        """Update learning progress for a domain."""
        history = self.domain_history[domain]
        if len(history) < 10:
            self.domain_lp[domain] = 0.5  # Default LP
            return
        
        # Compare recent surprise to earlier surprise
        recent = sum(history[-10:]) / 10
        earlier = sum(history[:-10]) / len(history[:-10])
        
        # LP is positive if surprise is decreasing (learning)
        lp = (earlier - recent) / max(earlier, 0.01)
        self.domain_lp[domain] = max(0, min(1, lp + 0.5))  # Normalize to [0, 1]
    
    def get_lp(self, domain: str) -> float:
        """Get learning progress for a domain."""
        return self.domain_lp.get(domain, 0.5)
    
    def get_high_lp_domains(self, n: int = 10) -> List[Tuple[str, float]]:
        """Get domains with highest learning progress."""
        sorted_domains = sorted(
            self.domain_lp.items(),
            key=lambda x: x[1],
            reverse=True
        )
        return sorted_domains[:n]
    
    def get_low_lp_domains(self, n: int = 10) -> List[Tuple[str, float]]:
        """Get domains with lowest learning progress (stagnant)."""
        sorted_domains = sorted(
            self.domain_lp.items(),
            key=lambda x: x[1]
        )
        return sorted_domains[:n]


class QuestionGenerator:
    """
    Generates curiosity questions based on learning progress.
    
    Prioritizes domains with high learning progress.
    """
    
    def __init__(self, lp_tracker: LearningProgressTracker):
        self.lp_tracker = lp_tracker
        self.question_templates = {
            "factual": [
                "What is the current {attribute} of {entity}?",
                "How does {entity_a} compare to {entity_b} in terms of {attribute}?",
                "What are the main characteristics of {entity}?",
            ],
            "causal": [
                "What causes {phenomenon}?",
                "What are the effects of {action} on {target}?",
                "How does {factor} influence {outcome}?",
            ],
            "procedural": [
                "How does {process} work?",
                "What are the steps involved in {task}?",
                "What is the mechanism behind {phenomenon}?",
            ],
            "counterfactual": [
                "What would happen if {condition}?",
                "How would {outcome} change if {factor} were different?",
            ],
        }
    
    async def generate_questions(
        self,
        domains: List[str],
        count: int = 10,
        prioritize_lp: bool = True
    ) -> List[CuriosityQuestion]:
        """Generate curiosity questions for domains."""
        questions = []
        
        # Sort domains by LP if prioritizing
        if prioritize_lp:
            domain_lps = [(d, self.lp_tracker.get_lp(d)) for d in domains]
            domain_lps.sort(key=lambda x: x[1], reverse=True)
            domains = [d for d, _ in domain_lps]
        
        for domain in domains:
            if len(questions) >= count:
                break
            
            # Generate question for this domain
            question = await self._generate_for_domain(domain)
            if question:
                questions.append(question)
        
        return questions
    
    async def _generate_for_domain(self, domain: str) -> Optional[CuriosityQuestion]:
        """Generate a question for a specific domain."""
        import uuid
        
        # In production, this would call an LLM to generate questions
        # For now, use templates
        template_type = random.choice(list(self.question_templates.keys()))
        template = random.choice(self.question_templates[template_type])
        
        # Simple placeholder replacement
        question = template.replace("{entity}", domain)
        question = question.replace("{phenomenon}", f"{domain} behavior")
        question = question.replace("{process}", f"{domain} process")
        
        # Determine claim type
        if template_type == "factual":
            claim_type = ClaimType.FACTUAL
        elif template_type == "causal":
            claim_type = ClaimType.SCIENTIFIC
        else:
            claim_type = ClaimType.GENERAL
        
        return CuriosityQuestion(
            question_id=str(uuid.uuid4()),
            question=question,
            domain=domain,
            claim_type=claim_type,
            requires_grounding=claim_type in GroundingPolicy.ALWAYS_GROUND,
            priority=self.lp_tracker.get_lp(domain),
            created_at=datetime.utcnow()
        )


class GroundedCuriosityEngine:
    """
    Main curiosity engine with mandatory grounding.
    
    Coordinates question generation, answer prediction,
    tool grounding, and learning progress tracking.
    """
    
    def __init__(self):
        self.grounding_policy = GroundingPolicy()
        self.lp_tracker = LearningProgressTracker()
        self.question_generator = QuestionGenerator(self.lp_tracker)
        self.question_queue: List[CuriosityQuestion] = []
        self.outcomes: List[LearningOutcome] = []
        
        logger.info("GroundedCuriosityEngine initialized")
    
    async def generate_questions(
        self,
        domains: List[str],
        count: int = 10
    ) -> List[CuriosityQuestion]:
        """Generate curiosity questions."""
        questions = await self.question_generator.generate_questions(
            domains, count
        )
        self.question_queue.extend(questions)
        return questions
    
    async def process_question(
        self,
        question: CuriosityQuestion,
        predict_fn,  # async fn(question) -> (prediction, confidence)
        ground_fn,   # async fn(question, prediction) -> GroundingResult
        nli_fn,      # async fn(prediction, ground_truth) -> (label, confidence, surprise)
    ) -> LearningOutcome:
        """
        Process a single curiosity question.
        
        1. Generate prediction
        2. Ground if required (20%+ minimum)
        3. Score surprise with NLI
        4. Update learning progress
        """
        # 1. Generate prediction
        prediction, pred_confidence = await predict_fn(question.question)
        
        # 2. Determine if grounding is needed
        should_ground = self.grounding_policy.should_ground(question.claim_type)
        
        grounding_result = None
        nli_label = "neutral"
        nli_confidence = 0.5
        surprise = 0.5
        
        if should_ground:
            # 3. Ground against external reality
            grounding_result = await ground_fn(question.question, prediction)
            
            # 4. Score surprise with NLI
            if grounding_result and grounding_result.result:
                nli_label, nli_confidence, surprise = await nli_fn(
                    prediction,
                    grounding_result.result
                )
        
        # 5. Calculate learning progress contribution
        lp_contribution = surprise * pred_confidence
        
        # 6. Update LP tracker
        self.lp_tracker.record_outcome(question.domain, surprise)
        
        # Create outcome
        outcome = LearningOutcome(
            question=question,
            prediction=prediction,
            prediction_confidence=pred_confidence,
            grounded=should_ground,
            grounding_result=grounding_result,
            nli_label=nli_label,
            nli_confidence=nli_confidence,
            surprise_score=surprise,
            learning_progress=lp_contribution,
            cost=0.01 if should_ground else 0.005  # Grounding costs more
        )
        
        self.outcomes.append(outcome)
        
        logger.info(
            f"Processed question: domain={question.domain}, "
            f"grounded={should_ground}, surprise={surprise:.2f}"
        )
        
        return outcome
    
    async def run_curiosity_loop(
        self,
        domains: List[str],
        question_count: int,
        predict_fn,
        ground_fn,
        nli_fn,
        budget_remaining: float
    ) -> List[LearningOutcome]:
        """
        Run a full curiosity loop.
        
        Generates questions, processes them, and returns outcomes.
        Respects budget constraints.
        """
        outcomes = []
        cost_so_far = 0.0
        
        # Generate questions
        questions = await self.generate_questions(domains, question_count)
        
        for question in questions:
            # Check budget
            estimated_cost = 0.01 if question.requires_grounding else 0.005
            if cost_so_far + estimated_cost > budget_remaining:
                logger.warning("Budget exhausted, stopping curiosity loop")
                break
            
            # Process question
            outcome = await self.process_question(
                question, predict_fn, ground_fn, nli_fn
            )
            
            outcomes.append(outcome)
            cost_so_far += outcome.cost
        
        logger.info(
            f"Curiosity loop complete: {len(outcomes)} questions, "
            f"${cost_so_far:.2f} spent, "
            f"grounding ratio: {self.grounding_policy.get_grounding_ratio():.1%}"
        )
        
        return outcomes
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get curiosity engine statistics."""
        return {
            "queue_size": len(self.question_queue),
            "outcomes_count": len(self.outcomes),
            "grounding_ratio": self.grounding_policy.get_grounding_ratio(),
            "high_lp_domains": self.lp_tracker.get_high_lp_domains(5),
            "low_lp_domains": self.lp_tracker.get_low_lp_domains(5),
            "average_surprise": (
                sum(o.surprise_score for o in self.outcomes) / len(self.outcomes)
                if self.outcomes else 0
            ),
            "total_cost": sum(o.cost for o in self.outcomes)
        }
    
    def reset_for_night_mode(self):
        """Reset counters for night mode processing."""
        self.grounding_policy.reset_counters()
        self.question_queue = []


# Singleton instance
_engine_instance: Optional[GroundedCuriosityEngine] = None


def get_curiosity_engine() -> GroundedCuriosityEngine:
    """Get or create the singleton curiosity engine."""
    global _engine_instance
    if _engine_instance is None:
        _engine_instance = GroundedCuriosityEngine()
    return _engine_instance
